#include <iostream>
#include <fstream>
#include <vector>
#include <valarray>
#include <string_view>
#include <charconv>
#include <random>
#include <algorithm>
#include <cmath>
#include <iomanip>

using namespace std;

class P1
{
public:

    valarray<double> samples;
    valarray<double> labels;
    valarray<double> weights;
    double bias;
    valarray<double> predictions;
    double l;
    double alpha = 0.00005;
    double inf = INFINITY;

    P1()
    {
        ifstream file("mnist_train.csv");
        mt19937 gen(1234);
        uniform_real_distribution<> dis(-1, 1);
        
        vector<double> tempLables;
        vector<double> tempSamples;
        while (!file.eof())
        {
            string line;
            getline(file, line);
            if (line[0] == '6' || line[0] == '9')
            {
                tempLables.emplace_back(line[0] == '6');
                auto && t = split(line);
                tempSamples.insert(tempSamples.end(), t.begin(), t.end());
            }
        }
        labels = valarray<double>(tempLables.data(), tempLables.size());
        samples = valarray<double>(tempSamples.data(), tempSamples.size());
        
        weights = valarray<double>{0.635677,0.224224,0.54272,0.72134,-0.698726,-0.602962,0.630326,-0.682369,-0.767724,-0.974185,-0.0263331,-0.337969,0.607906,-0.797744,-0.888013,-0.114675,-0.955712,-0.418543,-0.507211,0.476574,0.778452,0.974279,-0.765113,-0.212435,-0.0945404,0.0762957,0.581244,-0.0683273,-0.129335,0.138957,0.938518,-0.918888,0.0962512,-0.0748081,-0.246201,-0.342334,0.693885,0.337256,-0.865739,1.02466,0.406263,0.878822,0.845705,0.961018,-0.195266,-0.463105,-0.131714,0.533209,0.20206,-0.837277,0.411124,-0.669448,-0.935301,-0.343699,-0.05228,-0.863831,-0.234579,-0.762892,0.792693,0.528623,-0.246792,-0.947802,0.814479,0.583201,0.334258,-0.437605,-0.129148,0.464584,0.984351,0.65193,0.119626,-0.141506,-0.67546,-0.642802,0.604957,1.04371,0.425748,1.02057,0.403671,0.901259,0.168315,0.0633196,-0.662685,-0.682385,0.875931,0.436529,-0.0468355,0.76744,-0.182244,-0.646826,-0.73003,-0.134925,-0.703147,0.58246,1.09778,-0.0110755,0.196142,0.457179,0.926678,1.92128,-0.0565777,1.54371,-0.22201,1.19166,0.701566,-0.781353,-0.41709,0.897907,0.699393,-0.682392,0.387118,-0.266107,0.874945,0.22673,0.39871,0.00602108,0.448564,-0.691124,0.740715,0.692712,0.351138,-0.0376999,0.229709,1.26129,0.193539,0.091188,0.802837,0.283165,0.0111848,0.325306,0.087976,0.772752,0.388161,1.31737,0.822179,-0.710018,-0.460015,-0.738384,-0.64821,-0.140617,0.315538,0.131797,0.138216,0.30844,-0.177638,0.996384,-0.354421,0.995767,1.39937,0.179448,-0.248869,-1.00494,-0.100389,-0.829649,-0.498602,-0.712183,-0.38114,-0.6193,-0.0832346,0.775685,1.0129,0.248335,0.158413,-0.562032,-0.716598,0.290131,-0.982001,-0.163561,-0.987315,-0.151587,-0.274349,0.237604,-0.0469969,-0.780786,0.0578192,-0.441713,-0.0634179,0.253117,-0.597367,-0.483915,-1.31025,-0.686451,-0.803578,-0.649162,-0.905995,-1.63397,-0.414804,0.542577,-1.34954,0.181882,-0.0594109,0.269146,-0.033248,0.204812,-0.25806,-0.835392,0.0598398,0.518316,-0.964444,-0.929323,1.00572,0.0238209,-0.0343519,0.235083,-0.788427,0.142691,-0.728103,-0.465967,0.452368,-1.12686,-0.0290775,-0.972935,-1.46064,-1.15787,-1.2606,-0.835835,-1.38794,-1.4399,0.167932,-0.325575,0.370985,-0.333785,-0.842746,-0.550492,0.395252,-1.96717e-05,0.0085563,0.492426,0.882388,-0.403826,-0.681285,0.0417968,-0.390048,-1.0177,-0.234932,-0.184439,-0.462207,-0.984349,-0.8735,-1.22138,-1.58976,-1.2654,-0.537808,-0.688882,0.636497,-0.557554,-1.23595,0.56726,-0.81502,-0.423307,0.998146,0.0438452,0.645993,0.698629,0.371562,-0.93165,-0.82844,-0.899782,-0.396395,-0.379903,0.107484,-0.751408,-0.0555774,-0.773961,-0.0793243,-0.991334,-0.100529,-1.4904,-0.563012,-0.835759,-0.936782,-0.230313,-0.292284,-0.512018,-0.318496,0.184504,-0.0802448,-0.655066,0.564026,0.086329,0.563132,0.187089,-0.781661,-0.980698,-1.01077,-0.62004,-0.205602,-0.488218,-0.39246,-0.598878,-1.01419,0.177744,0.542599,0.121109,0.317546,-1.38123,-0.267686,-1.16523,-0.663434,-0.747051,-0.999639,0.618023,-0.318427,0.331565,-0.123484,-0.357927,-0.975681,-0.597947,0.528903,-0.545423,-0.23043,-0.0284347,-0.131568,-0.705203,-0.461802,-0.623269,-0.331103,-0.260791,0.290827,-0.435136,-0.477929,-0.125675,-0.139234,-1.18796,-0.220107,-0.0507659,-0.899977,-0.644375,-0.138768,0.042785,0.207424,0.38039,0.364471,-0.438236,-0.472336,-0.177395,-0.345634,0.274323,-0.610739,-0.129484,-0.722254,-0.708778,0.275051,0.516051,-0.311981,0.465682,0.199579,-0.552819,0.814443,0.594826,0.44315,-0.215273,-0.421827,-1.10138,-0.128234,-0.0128442,-0.604638,0.47818,-0.0397187,0.460316,0.580978,0.656662,0.251207,-0.2701,-0.933498,-0.392285,-0.644954,-0.541695,-1.05303,-0.887127,-1.35633,0.0329124,-0.738068,0.561097,-0.199847,1.1082,1.10664,0.134144,-0.596443,0.111551,-0.767244,0.0129334,-0.50365,-0.0885979,0.244797,0.481115,-0.913466,1.70702,-0.0140393,-0.613949,0.736196,0.727728,0.519647,0.390719,-0.0311964,0.0927939,0.504669,-1.21928,-2.90898e-05,-0.676381,0.0264874,-0.15557,0.108465,0.212946,0.389301,0.726257,-0.0560903,0.579554,-0.270475,-1.13016,0.266395,-0.571952,-0.280537,-0.841068,1.17671,1.37012,0.349667,-0.211422,-0.11961,-0.12845,0.182967,0.463195,-0.879559,-0.296511,-0.886293,-0.044544,0.452096,-0.559686,-0.0129979,-0.136852,0.423854,0.981194,0.218281,0.271622,-0.69179,-0.0427633,-0.666275,-0.21897,-0.358433,0.0795144,-0.898654,-0.195132,0.223249,-0.199859,0.917783,-0.585495,0.321362,-0.140688,-0.409543,-0.601013,-0.380312,0.0689134,0.121914,0.110186,0.714293,-0.316871,0.778657,0.122844,0.945962,1.47321,-0.0841323,0.0911869,0.842301,0.888101,0.274511,-0.171622,0.701157,0.457416,-0.42674,0.802383,0.0713582,0.665923,0.670894,-0.569628,-0.292175,0.174065,0.0670594,-0.615319,-0.162037,-0.618145,-0.976505,-0.493462,0.873211,0.385858,0.515414,0.927173,0.371709,0.336803,0.793063,-0.00445873,-0.131706,0.149933,-0.081059,0.310661,1.06622,0.582565,0.207691,1.10999,0.573733,-0.460546,0.11555,0.692011,-0.697053,-0.525937,0.447511,-0.477469,-0.911572,-0.959829,0.821651,-0.190424,-0.133155,0.139068,1.26462,0.354418,1.36837,1.49346,1.16349,-0.0480537,-0.701715,-0.233201,0.405681,-0.78745,1.04243,-0.00602153,1.28772,1.07448,1.16504,0.48508,-1.13881,0.684683,-0.603796,-0.421949,0.75809,0.0465499,0.542769,-0.355975,0.674878,0.375263,0.630334,0.554177,0.944357,1.18264,1.22197,1.04904,1.16457,-0.0511942,1.08318,0.561255,0.11507,0.109515,0.701297,-0.00184114,-0.481966,0.158337,0.51435,0.137769,-1.17405,0.639804,0.492982,0.985404,-0.930985,0.893902,0.561424,0.145399,-0.652571,-0.301257,0.678802,1.07577,0.216113,0.274906,1.22261,0.702173,2.18113,0.445819,0.247196,0.0431116,0.586079,-0.479163,-0.64616,0.717824,0.354458,-0.536728,0.441471,-1.19419,-1.00327,0.376565,-0.637623,0.945366,-0.910102,-0.856868,0.306443,0.657578,0.756413,-0.110953,0.59596,0.352232,-0.133228,0.137078,1.38555,1.06693,0.942257,0.876982,1.17746,0.439063,-0.125506,0.578743,0.239986,0.961177,0.666362,0.506197,0.137262,0.498613,-0.534583,0.902651,0.4871,-0.892057,0.107672,0.0938362,0.957509,0.460108,-0.890942,-0.136839,-1.36126,-0.605454,0.558236,0.125777,0.49453,0.345846,0.914629,1.38257,0.266678,-0.182631,-0.246611,0.456062,0.916467,0.112898,0.0538906,-0.123314,0.0757032,-1.20682,-1.13438,-1.008,0.296299,-0.857958,-0.656837,-0.93256,-0.153487,0.445704,0.494329,-0.103993,0.306303,0.115057,0.0119569,-1.58132,-0.603713,-0.942081,0.211952,-0.70706,1.02485,-0.271554,0.285315,-1.01972,1.38702,1.02379,-0.398352,-0.554694,-1.03047,-0.835312,0.286677,0.371654,-0.0434361,0.484373,0.135416,0.395786,0.203192,-0.949172,-0.0891615,-0.707679,0.254278,0.045766,-0.111996,-0.552186,-0.461387,-0.909893,-0.409159,-1.94682,-0.653386,0.402745,0.022161,0.28748,-0.549742,-1.35364,-0.647286,-0.876584,-0.559596,-0.688072,0.610531,0.683679,0.630497,0.704631,0.797588,-0.390521,0.419529,-0.261104,0.260083,0.682115,-0.31718,-0.407867,-0.95264,0.245777,-1.67209,-0.61751,0.177132,-0.331996,0.128494,-1.38418,-0.0174308,0.0614073,-0.765017,-1.065,-0.0504523,-0.495244,-0.848468,0.190674,-0.946991,-0.608292,-0.463012,0.967839,0.239529,-0.354232,-0.952066,0.271613,0.973321,-0.999023,-0.780751,-0.841702,-0.351449,0.102164,0.844129,-0.356554,-0.156055,-0.511161,-0.245772,-0.115619,-0.267714,-0.992389,-0.413462,0.5332,-0.939004,-0.163053,-0.675539,-0.260748,0.308042,0.746883,-0.641746,-0.327222,0.766967,-0.478134,0.618105,-0.373097,-0.923406,-0.824023,-0.680612,-0.176748,-0.561331,0.724063,-0.227042,-0.154404,-0.370944,0.73129,-0.837998,0.892004,0.798857,0.778205,0.0658686,0.717089,0.413515,0.369236,0.950811,-0.48494,0.260965,0.955943,0.0516991,0.167837};
        bias = -0.373764;
        predictions = valarray<double>(labels.size());
        // for (auto && w : weights)
        //     cout << w << ",";

        

        int count = 0;
        for (size_t i = 0; i < predictions.size(); i++)
            predictions[i] = func(samples[slice(i * 784, 784, 1)]);
        do
        {
            l = loss();
            for (size_t i = 0; i < weights.size(); i++)
                weights[i] = weights[i] - alpha * ((predictions - labels) * 
                    samples[slice(i, labels.size(), weights.size())]).sum();
            bias = bias - alpha * (predictions - labels).sum();
            for (size_t i = 0; i < predictions.size(); i++)
                predictions[i] = func(samples[slice(i * 784, 784, 1)]);
            cout << l << endl;
            count++;
            alpha = 0.00005 / sqrt(count);
        // } while (loss() > 8.37);
        } while (abs(loss() - l) > 0.0001 || loss() == INFINITY);
    }

    vector<double> split(const string & str)
    {
        string_view s(str.c_str());
        s = s.substr(2); // get rid of the label
        vector<double> ret(784);
        size_t start = 0;
        for (size_t i = 0; i < 784; i++)
        {
            int temp;
            string_view token = s.substr(start, s.find(",", start) - start);
            from_chars(token.data(), token.data() + token.size(), temp);
            start += token.size() + 1;
            ret[i] = temp / 255.0;
        }
        return ret;
    }

    double func(const valarray<double> & data)
    {
        double d = 1.0 / (1 + exp(-((data * weights).sum() + bias)));
        return d;
    }

    double loss()
    {
        double ret = 0;
        for (size_t i = 0; i < labels.size(); i++)
        {
            double v = predictions[i];
            if (v == 0 || 1 - v == 0)
                return INFINITY;
            ret += labels[i] * log(v) + (1 - labels[i]) * log(1 - v);
        }
        return -ret;
    }

    void print()
    {
        for (auto &&i : weights)
            cout << i << ",";
        cout << bias << endl;
    }
};

int main()
{
    P1 p;
    p.print();
}